{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"! wget https://raw.githubusercontent.com/sighsmile/conlleval/master/conlleval.py","metadata":{"execution":{"iopub.status.busy":"2023-11-11T02:08:19.447732Z","iopub.execute_input":"2023-11-11T02:08:19.448089Z","iopub.status.idle":"2023-11-11T02:08:20.521414Z","shell.execute_reply.started":"2023-11-11T02:08:19.448060Z","shell.execute_reply":"2023-11-11T02:08:20.520139Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"--2023-11-11 02:08:20--  https://raw.githubusercontent.com/sighsmile/conlleval/master/conlleval.py\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 7502 (7.3K) [text/plain]\nSaving to: ‘conlleval.py.1’\n\nconlleval.py.1      100%[===================>]   7.33K  --.-KB/s    in 0s      \n\n2023-11-11 02:08:20 (78.2 MB/s) - ‘conlleval.py.1’ saved [7502/7502]\n\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nimport itertools\nfrom collections import Counter\nfrom torch.utils.data import DataLoader\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.utils.data import TensorDataset\nfrom conlleval import evaluate\nfrom tqdm import tqdm\ntorch.manual_seed(1)\nnp.random.seed(1)","metadata":{"execution":{"iopub.status.busy":"2023-11-11T02:08:20.524123Z","iopub.execute_input":"2023-11-11T02:08:20.524743Z","iopub.status.idle":"2023-11-11T02:08:20.531365Z","shell.execute_reply.started":"2023-11-11T02:08:20.524711Z","shell.execute_reply":"2023-11-11T02:08:20.530273Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"import datasets\n\ndataset = datasets.load_dataset(\"conll2003\")","metadata":{"execution":{"iopub.status.busy":"2023-11-11T02:08:20.532511Z","iopub.execute_input":"2023-11-11T02:08:20.532770Z","iopub.status.idle":"2023-11-11T02:08:20.863404Z","shell.execute_reply.started":"2023-11-11T02:08:20.532747Z","shell.execute_reply":"2023-11-11T02:08:20.862555Z"},"trusted":true},"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f07fb7634fbc4a5c922e2cc1e532f906"}},"metadata":{}}]},{"cell_type":"code","source":"dataset","metadata":{"execution":{"iopub.status.busy":"2023-11-11T02:08:20.865613Z","iopub.execute_input":"2023-11-11T02:08:20.865892Z","iopub.status.idle":"2023-11-11T02:08:20.871793Z","shell.execute_reply.started":"2023-11-11T02:08:20.865867Z","shell.execute_reply":"2023-11-11T02:08:20.870912Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n        num_rows: 14042\n    })\n    validation: Dataset({\n        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n        num_rows: 3251\n    })\n    test: Dataset({\n        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n        num_rows: 3454\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"word_freq = Counter(itertools.chain(*dataset['train']['tokens']))\n\nword_freq = {\n    word: frequency\n    for word, frequency in word_freq.items()\n    if frequency >= 2\n}\n\nw2ids = {\n    word: index\n    for index, word in enumerate(word_freq.keys(), start=2)\n}\n\nw2ids['[PAD]'] = 0\nw2ids['[UNK]'] = 1\n\n# Preprocess the dataset using the provided word2idx mapping\ndef preprocess_sample(sample):\n    # Convert tokens to their respective indexes using w2ids\n    input_ids = [w2ids.get(word, w2ids['[UNK]']) for word in sample['tokens']]\n    \n    # Update the sample with 'input_ids'\n    sample['input_ids'] = input_ids\n    \n    # Remove 'pos tags' and 'chunk tags'\n    sample.pop('pos_tags', None)\n    sample.pop('chunk_tags', None)\n    sample.pop('id', None)\n    \n    # Rename 'ner_tags' to 'labels'\n    sample['labels'] = sample.pop('ner_tags')\n    \n    return sample\n\n# Apply the preprocessing using .map() function\npreprocessed_dataset = dataset.map(preprocess_sample)","metadata":{"execution":{"iopub.status.busy":"2023-11-11T02:08:20.872992Z","iopub.execute_input":"2023-11-11T02:08:20.873276Z","iopub.status.idle":"2023-11-11T02:08:25.243193Z","shell.execute_reply.started":"2023-11-11T02:08:20.873230Z","shell.execute_reply":"2023-11-11T02:08:25.242446Z"},"trusted":true},"execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/14042 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9bbd5604a424076976c75ad55ef13d9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3251 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"844048dca4ce45408dd69ca59062f03b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3454 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"328ab83be1f642caa5e500579cd1c68d"}},"metadata":{}}]},{"cell_type":"code","source":"preprocessed_dataset","metadata":{"execution":{"iopub.status.busy":"2023-11-11T02:08:25.244439Z","iopub.execute_input":"2023-11-11T02:08:25.244865Z","iopub.status.idle":"2023-11-11T02:08:25.250810Z","shell.execute_reply.started":"2023-11-11T02:08:25.244783Z","shell.execute_reply":"2023-11-11T02:08:25.250028Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['tokens', 'input_ids', 'labels'],\n        num_rows: 14042\n    })\n    validation: Dataset({\n        features: ['tokens', 'input_ids', 'labels'],\n        num_rows: 3251\n    })\n    test: Dataset({\n        features: ['tokens', 'input_ids', 'labels'],\n        num_rows: 3454\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"# Assuming you have a preprocessed train, test, and validation dataset\ntrain_dataset = preprocessed_dataset['train']\ntest_dataset = preprocessed_dataset['test']\nvalidation_dataset = preprocessed_dataset['validation']\n\n# Define the special label for 'PAD'\nPAD_LABEL = 9\n\n# Create custom collate function for DataLoader\ndef custom_collate(batch):\n    # Separate input_ids and labels\n    input_ids = [torch.tensor(item['input_ids']) for item in batch]\n    labels = [torch.tensor(item['labels']) for item in batch]\n    input_id_orig = [len(terms) for terms in input_ids]\n    \n    # Pad input_ids and labels using pad_sequence\n    input_ids = pad_sequence(input_ids, batch_first=True, padding_value=w2ids['[PAD]'])\n    labels = pad_sequence(labels, batch_first=True, padding_value=PAD_LABEL)\n\n    return {'input_ids': input_ids, 'labels': labels, 'input_id_orig': input_id_orig}\n\n# Create DataLoader for train, test, and validation datasets\nbatch_size = 32  # You can adjust the batch size as needed\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=custom_collate, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=len(test_dataset), collate_fn=custom_collate)\nvalidation_loader = DataLoader(validation_dataset, batch_size=batch_size, collate_fn=custom_collate)","metadata":{"execution":{"iopub.status.busy":"2023-11-11T02:08:25.252382Z","iopub.execute_input":"2023-11-11T02:08:25.252740Z","iopub.status.idle":"2023-11-11T02:08:25.271471Z","shell.execute_reply.started":"2023-11-11T02:08:25.252708Z","shell.execute_reply":"2023-11-11T02:08:25.270796Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# Define the BiLSTM model\nclass BiLSTMModel(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, num_lstm_layers, lstm_hidden_dim, linear_output_dim, tagset_size):\n        super(BiLSTMModel, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.bilstm = nn.LSTM(embedding_dim, lstm_hidden_dim, num_layers=num_lstm_layers, bidirectional=True, batch_first=True)\n        self.linear = nn.Linear(2 * lstm_hidden_dim, linear_output_dim)\n        self.elu = nn.ELU()\n        self.classifier = nn.Linear(linear_output_dim, tagset_size)\n        self.dropout = nn.Dropout(p=0.33)  # Adjust the dropout rate as needed\n        \n    def forward(self, input_ids):\n        embeddings = self.embedding(input_ids)\n        lstm_out, _ = self.bilstm(embeddings)\n        lstm_out = self.dropout(lstm_out)  # Apply dropout to the LSTM output\n        linear_out = self.linear(lstm_out)\n        elu_out = self.elu(linear_out)\n        logits = self.classifier(elu_out)\n        return logits\n    \n# Check for GPU availability\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n# Define hyperparameters\nvocab_size = len(w2ids)\nprint(vocab_size)\ntagset_size = 9\nembedding_dim = 100\nnum_lstm_layers = 1\nlstm_hidden_dim = 256\nlinear_output_dim = 128\nlearning_rate = 0.01\nnum_epochs = 100  # You can adjust the number of epochs\n\n# Create BiLSTM model\nmodel = BiLSTMModel(vocab_size, embedding_dim, num_lstm_layers, lstm_hidden_dim, linear_output_dim, tagset_size)\nmodel.to(device)\n\n# Define loss function and optimizer\ncriterion = nn.CrossEntropyLoss(ignore_index=9)  # Ignore the pad token\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n# Early stopping parameters\npatience = 5  # Number of epochs with no improvement before stopping\nbest_validation_loss = float('inf')\ncounter = 0\nbest_validation_loss = 0\n\nidx_to_tag = {0:'O', 1:'B-PER', 2:'I-PER', 3:'B-ORG', 4:'I-ORG', 5:'B-LOC', 6:'I-LOC', 7:'B-MISC', 8:'I-MISC'}\n\ndef train(model, train_loader, optimizer, criterion, idx_to_tag):\n    model.train()\n\n    for batch in train_loader:\n        input_ids, labels = batch['input_ids'].to(device, dtype=torch.long), batch['labels'].to(device, dtype=torch.long)\n        optimizer.zero_grad()\n        logits = model(input_ids)\n        loss = criterion(logits.view(-1, tagset_size), labels.view(-1))\n        loss.backward()\n        optimizer.step()\n\ndef eval_model(model, loader, idx_to_tag):\n    model.eval()\n\n    with torch.no_grad():\n        preds = []\n        real_labels = []\n        for batch in tqdm(loader):\n            val_input_ids, val_labels = batch['input_ids'].to(device, dtype=torch.long), batch['labels'].to(device, dtype=torch.long)\n            logits = model(val_input_ids)\n\n            predictions = torch.argmax(logits, dim=-1).cpu().numpy().tolist()\n            real_val_labels = val_labels.cpu().numpy().tolist()\n            \n            for temp in range(len(batch['input_id_orig'])):\n                preds.append(predictions[temp][:batch['input_id_orig'][temp]])\n                real_labels.append(real_val_labels[temp][:batch['input_id_orig'][temp]])\n            \n    preds = list(itertools.chain(*preds))\n    real_labels = list(itertools.chain(*real_labels))\n\n    preds = [idx_to_tag[prediction] for prediction in preds]\n    real_labels = [idx_to_tag[label] for label in real_labels]\n\n    # Evaluate on validation data and print the results\n    metrics = evaluate(real_labels, preds)\n\n    return metrics\n\n# Training loop\nfor epoch in range(num_epochs):\n    train(model, train_loader, optimizer, criterion, idx_to_tag)\n\n    print(f\"Epoch {epoch+1}:\")\n    # Set the model to evaluation mode for validation\n    val_loss = eval_model(model, validation_loader, idx_to_tag)\n\n    # Early stopping check\n    if val_loss[2] > best_validation_loss:\n        best_validation_loss = val_loss[2]\n        counter = 0\n        # Save the model as .pt\n        torch.save(model.state_dict(), 'task1_model.pt')\n    else:\n        counter += 1\n        if counter >= patience:\n            print(f'Early stopping at epoch {epoch+1}')\n            print(f'Best F1 score: {best_validation_loss}')\n            break","metadata":{"execution":{"iopub.status.busy":"2023-11-11T02:08:25.272744Z","iopub.execute_input":"2023-11-11T02:08:25.273071Z","iopub.status.idle":"2023-11-11T02:10:10.798062Z","shell.execute_reply.started":"2023-11-11T02:08:25.273048Z","shell.execute_reply":"2023-11-11T02:10:10.797147Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"11984\nEpoch 1:\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 102/102 [00:00<00:00, 150.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"processed 51362 tokens with 5942 phrases; found: 5459 phrases; correct: 4249.\naccuracy:  74.11%; (non-O)\naccuracy:  95.20%; precision:  77.83%; recall:  71.51%; FB1:  74.54\n              LOC: precision:  86.46%; recall:  75.07%; FB1:  80.36  1595\n             MISC: precision:  82.49%; recall:  70.50%; FB1:  76.02  788\n              ORG: precision:  64.90%; recall:  63.83%; FB1:  64.36  1319\n              PER: precision:  77.63%; recall:  74.05%; FB1:  75.80  1757\nEpoch 2:\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 102/102 [00:00<00:00, 149.51it/s]\n","output_type":"stream"},{"name":"stdout","text":"processed 51362 tokens with 5942 phrases; found: 5661 phrases; correct: 4561.\naccuracy:  79.93%; (non-O)\naccuracy:  95.87%; precision:  80.57%; recall:  76.76%; FB1:  78.62\n              LOC: precision:  86.68%; recall:  85.36%; FB1:  86.01  1809\n             MISC: precision:  86.06%; recall:  73.64%; FB1:  79.37  789\n              ORG: precision:  71.97%; recall:  66.82%; FB1:  69.30  1245\n              PER: precision:  78.00%; recall:  76.98%; FB1:  77.49  1818\nEpoch 3:\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 102/102 [00:00<00:00, 144.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"processed 51362 tokens with 5942 phrases; found: 5116 phrases; correct: 4329.\naccuracy:  75.74%; (non-O)\naccuracy:  95.76%; precision:  84.62%; recall:  72.85%; FB1:  78.30\n              LOC: precision:  89.49%; recall:  80.68%; FB1:  84.86  1656\n             MISC: precision:  87.25%; recall:  76.46%; FB1:  81.50  808\n              ORG: precision:  77.44%; recall:  65.77%; FB1:  71.13  1139\n              PER: precision:  83.28%; recall:  68.40%; FB1:  75.11  1513\nEpoch 4:\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 102/102 [00:00<00:00, 145.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"processed 51362 tokens with 5942 phrases; found: 5520 phrases; correct: 4517.\naccuracy:  79.22%; (non-O)\naccuracy:  95.92%; precision:  81.83%; recall:  76.02%; FB1:  78.82\n              LOC: precision:  90.83%; recall:  80.84%; FB1:  85.54  1635\n             MISC: precision:  73.96%; recall:  77.33%; FB1:  75.61  964\n              ORG: precision:  71.36%; recall:  73.01%; FB1:  72.17  1372\n              PER: precision:  86.51%; recall:  72.75%; FB1:  79.03  1549\nEpoch 5:\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 102/102 [00:00<00:00, 143.87it/s]\n","output_type":"stream"},{"name":"stdout","text":"processed 51362 tokens with 5942 phrases; found: 5415 phrases; correct: 4490.\naccuracy:  78.09%; (non-O)\naccuracy:  95.92%; precision:  82.92%; recall:  75.56%; FB1:  79.07\n              LOC: precision:  86.35%; recall:  85.03%; FB1:  85.68  1809\n             MISC: precision:  84.63%; recall:  72.23%; FB1:  77.94  787\n              ORG: precision:  76.64%; recall:  70.69%; FB1:  73.55  1237\n              PER: precision:  83.06%; recall:  71.34%; FB1:  76.75  1582\nEpoch 6:\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 102/102 [00:00<00:00, 144.38it/s]\n","output_type":"stream"},{"name":"stdout","text":"processed 51362 tokens with 5942 phrases; found: 5347 phrases; correct: 4455.\naccuracy:  78.17%; (non-O)\naccuracy:  95.92%; precision:  83.32%; recall:  74.97%; FB1:  78.93\n              LOC: precision:  92.93%; recall:  77.95%; FB1:  84.78  1541\n             MISC: precision:  82.29%; recall:  75.60%; FB1:  78.80  847\n              ORG: precision:  70.49%; recall:  73.75%; FB1:  72.08  1403\n              PER: precision:  85.93%; recall:  72.58%; FB1:  78.69  1556\nEpoch 7:\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 102/102 [00:00<00:00, 142.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"processed 51362 tokens with 5942 phrases; found: 5777 phrases; correct: 4651.\naccuracy:  81.83%; (non-O)\naccuracy:  96.02%; precision:  80.51%; recall:  78.27%; FB1:  79.38\n              LOC: precision:  87.77%; recall:  84.81%; FB1:  86.27  1775\n             MISC: precision:  82.38%; recall:  76.57%; FB1:  79.37  857\n              ORG: precision:  68.94%; recall:  71.51%; FB1:  70.20  1391\n              PER: precision:  81.41%; recall:  77.52%; FB1:  79.42  1754\nEpoch 8:\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 102/102 [00:00<00:00, 145.71it/s]\n","output_type":"stream"},{"name":"stdout","text":"processed 51362 tokens with 5942 phrases; found: 5881 phrases; correct: 4675.\naccuracy:  81.18%; (non-O)\naccuracy:  96.05%; precision:  79.49%; recall:  78.68%; FB1:  79.08\n              LOC: precision:  81.68%; recall:  88.08%; FB1:  84.76  1981\n             MISC: precision:  78.88%; recall:  76.14%; FB1:  77.48  890\n              ORG: precision:  73.79%; recall:  70.54%; FB1:  72.13  1282\n              PER: precision:  81.54%; recall:  76.49%; FB1:  78.94  1728\nEpoch 9:\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 102/102 [00:00<00:00, 146.59it/s]\n","output_type":"stream"},{"name":"stdout","text":"processed 51362 tokens with 5942 phrases; found: 5821 phrases; correct: 4696.\naccuracy:  81.03%; (non-O)\naccuracy:  95.95%; precision:  80.67%; recall:  79.03%; FB1:  79.84\n              LOC: precision:  87.14%; recall:  86.66%; FB1:  86.90  1827\n             MISC: precision:  84.98%; recall:  77.33%; FB1:  80.98  839\n              ORG: precision:  82.03%; recall:  67.04%; FB1:  73.78  1096\n              PER: precision:  72.46%; recall:  81.00%; FB1:  76.49  2059\nEpoch 10:\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 102/102 [00:00<00:00, 134.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"processed 51362 tokens with 5942 phrases; found: 5086 phrases; correct: 4376.\naccuracy:  76.08%; (non-O)\naccuracy:  95.77%; precision:  86.04%; recall:  73.65%; FB1:  79.36\n              LOC: precision:  93.82%; recall:  80.13%; FB1:  86.44  1569\n             MISC: precision:  85.71%; recall:  78.09%; FB1:  81.73  840\n              ORG: precision:  78.60%; recall:  69.57%; FB1:  73.81  1187\n              PER: precision:  83.96%; recall:  67.92%; FB1:  75.09  1490\nEpoch 11:\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 102/102 [00:00<00:00, 146.24it/s]\n","output_type":"stream"},{"name":"stdout","text":"processed 51362 tokens with 5942 phrases; found: 5605 phrases; correct: 4615.\naccuracy:  80.59%; (non-O)\naccuracy:  96.20%; precision:  82.34%; recall:  77.67%; FB1:  79.93\n              LOC: precision:  89.59%; recall:  83.40%; FB1:  86.38  1710\n             MISC: precision:  81.92%; recall:  76.68%; FB1:  79.22  863\n              ORG: precision:  73.80%; recall:  69.95%; FB1:  71.82  1271\n              PER: precision:  81.66%; recall:  78.07%; FB1:  79.82  1761\nEpoch 12:\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 102/102 [00:00<00:00, 145.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"processed 51362 tokens with 5942 phrases; found: 5662 phrases; correct: 4595.\naccuracy:  80.08%; (non-O)\naccuracy:  96.02%; precision:  81.16%; recall:  77.33%; FB1:  79.20\n              LOC: precision:  85.86%; recall:  83.61%; FB1:  84.72  1789\n             MISC: precision:  88.69%; recall:  75.70%; FB1:  81.69  787\n              ORG: precision:  66.01%; recall:  73.01%; FB1:  69.33  1483\n              PER: precision:  86.21%; recall:  75.03%; FB1:  80.23  1603\nEpoch 13:\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 102/102 [00:00<00:00, 148.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"processed 51362 tokens with 5942 phrases; found: 5173 phrases; correct: 4449.\naccuracy:  77.07%; (non-O)\naccuracy:  95.88%; precision:  86.00%; recall:  74.87%; FB1:  80.05\n              LOC: precision:  89.98%; recall:  85.52%; FB1:  87.69  1746\n             MISC: precision:  84.10%; recall:  76.90%; FB1:  80.34  843\n              ORG: precision:  80.32%; recall:  64.21%; FB1:  71.36  1072\n              PER: precision:  86.51%; recall:  71.01%; FB1:  78.00  1512\nEpoch 14:\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 102/102 [00:00<00:00, 146.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"processed 51362 tokens with 5942 phrases; found: 5707 phrases; correct: 4537.\naccuracy:  78.77%; (non-O)\naccuracy:  95.85%; precision:  79.50%; recall:  76.35%; FB1:  77.90\n              LOC: precision:  85.54%; recall:  86.01%; FB1:  85.78  1847\n             MISC: precision:  86.57%; recall:  75.49%; FB1:  80.65  804\n              ORG: precision:  64.19%; recall:  71.36%; FB1:  67.58  1491\n              PER: precision:  83.32%; recall:  70.79%; FB1:  76.55  1565\nEpoch 15:\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 102/102 [00:00<00:00, 147.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"processed 51362 tokens with 5942 phrases; found: 5692 phrases; correct: 4599.\naccuracy:  80.17%; (non-O)\naccuracy:  95.98%; precision:  80.80%; recall:  77.40%; FB1:  79.06\n              LOC: precision:  81.34%; recall:  87.53%; FB1:  84.32  1977\n             MISC: precision:  83.69%; recall:  75.70%; FB1:  79.50  834\n              ORG: precision:  74.01%; recall:  68.16%; FB1:  70.96  1235\n              PER: precision:  83.78%; recall:  74.86%; FB1:  79.07  1646\nEpoch 16:\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 102/102 [00:00<00:00, 147.43it/s]\n","output_type":"stream"},{"name":"stdout","text":"processed 51362 tokens with 5942 phrases; found: 5564 phrases; correct: 4647.\naccuracy:  80.37%; (non-O)\naccuracy:  96.15%; precision:  83.52%; recall:  78.21%; FB1:  80.78\n              LOC: precision:  89.28%; recall:  85.68%; FB1:  87.44  1763\n             MISC: precision:  87.12%; recall:  77.01%; FB1:  81.75  815\n              ORG: precision:  74.26%; recall:  72.48%; FB1:  73.36  1309\n              PER: precision:  82.95%; recall:  75.52%; FB1:  79.06  1677\nEpoch 17:\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 102/102 [00:00<00:00, 143.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"processed 51362 tokens with 5942 phrases; found: 5534 phrases; correct: 4614.\naccuracy:  80.43%; (non-O)\naccuracy:  96.11%; precision:  83.38%; recall:  77.65%; FB1:  80.41\n              LOC: precision:  91.81%; recall:  82.96%; FB1:  87.16  1660\n             MISC: precision:  86.64%; recall:  76.68%; FB1:  81.36  816\n              ORG: precision:  70.05%; recall:  74.12%; FB1:  72.03  1419\n              PER: precision:  84.75%; recall:  75.41%; FB1:  79.80  1639\nEpoch 18:\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 102/102 [00:00<00:00, 148.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"processed 51362 tokens with 5942 phrases; found: 5679 phrases; correct: 4525.\naccuracy:  79.03%; (non-O)\naccuracy:  95.86%; precision:  79.68%; recall:  76.15%; FB1:  77.88\n              LOC: precision:  92.04%; recall:  79.26%; FB1:  85.17  1582\n             MISC: precision:  84.28%; recall:  75.60%; FB1:  79.70  827\n              ORG: precision:  61.13%; recall:  74.35%; FB1:  67.09  1631\n              PER: precision:  83.89%; recall:  74.65%; FB1:  79.00  1639\nEpoch 19:\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 102/102 [00:00<00:00, 141.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"processed 51362 tokens with 5942 phrases; found: 5586 phrases; correct: 4549.\naccuracy:  79.91%; (non-O)\naccuracy:  96.04%; precision:  81.44%; recall:  76.56%; FB1:  78.92\n              LOC: precision:  88.91%; recall:  82.47%; FB1:  85.57  1704\n             MISC: precision:  82.48%; recall:  76.57%; FB1:  79.42  856\n              ORG: precision:  69.58%; recall:  70.62%; FB1:  70.10  1361\n              PER: precision:  82.94%; recall:  74.97%; FB1:  78.76  1665\nEpoch 20:\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 102/102 [00:00<00:00, 131.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"processed 51362 tokens with 5942 phrases; found: 5358 phrases; correct: 4524.\naccuracy:  78.94%; (non-O)\naccuracy:  96.07%; precision:  84.43%; recall:  76.14%; FB1:  80.07\n              LOC: precision:  91.22%; recall:  83.18%; FB1:  87.02  1675\n             MISC: precision:  84.32%; recall:  77.01%; FB1:  80.50  842\n              ORG: precision:  75.22%; recall:  71.07%; FB1:  73.08  1267\n              PER: precision:  84.69%; recall:  72.37%; FB1:  78.04  1574\nEpoch 21:\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 102/102 [00:00<00:00, 146.61it/s]\n","output_type":"stream"},{"name":"stdout","text":"processed 51362 tokens with 5942 phrases; found: 5364 phrases; correct: 4419.\naccuracy:  77.36%; (non-O)\naccuracy:  95.74%; precision:  82.38%; recall:  74.37%; FB1:  78.17\n              LOC: precision:  91.72%; recall:  80.19%; FB1:  85.56  1606\n             MISC: precision:  87.77%; recall:  74.73%; FB1:  80.73  785\n              ORG: precision:  67.68%; recall:  73.38%; FB1:  70.41  1454\n              PER: precision:  83.81%; recall:  69.11%; FB1:  75.75  1519\nEarly stopping at epoch 21\nBest F1 score: 80.77524769685381\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Validation Results Task 1","metadata":{}},{"cell_type":"code","source":"# Load the state dictionary\nmodel.load_state_dict(torch.load('task1_model.pt'))\nmodel.eval()\n# Move the model to the same device as the input data (cuda or cpu)\nmodel.to(device)\n\nwith torch.no_grad():\n    preds = []\n    real_labels = []\n    for batch in tqdm(validation_loader):\n        val_input_ids, val_labels = batch['input_ids'].to(device, dtype=torch.long), batch['labels'].to(device, dtype=torch.long)\n        logits = model(val_input_ids)\n\n        predictions = torch.argmax(logits, dim=-1).cpu().numpy().tolist()\n        real_val_labels = val_labels.cpu().numpy().tolist()\n\n        for temp in range(len(batch['input_id_orig'])):\n            preds.append(predictions[temp][:batch['input_id_orig'][temp]])\n            real_labels.append(real_val_labels[temp][:batch['input_id_orig'][temp]])\n            \npreds = list(itertools.chain(*preds))\nreal_labels = list(itertools.chain(*real_labels))\n\npreds = [idx_to_tag[prediction] for prediction in preds]\nreal_labels = [idx_to_tag[label] for label in real_labels]\n\n# Evaluate on validation data and print the results\nmetrics = evaluate(real_labels, preds)","metadata":{"execution":{"iopub.status.busy":"2023-11-11T02:10:10.799401Z","iopub.execute_input":"2023-11-11T02:10:10.800043Z","iopub.status.idle":"2023-11-11T02:10:11.697042Z","shell.execute_reply.started":"2023-11-11T02:10:10.800006Z","shell.execute_reply":"2023-11-11T02:10:11.696075Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stderr","text":"100%|██████████| 102/102 [00:00<00:00, 144.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"processed 51362 tokens with 5942 phrases; found: 5564 phrases; correct: 4647.\naccuracy:  80.37%; (non-O)\naccuracy:  96.15%; precision:  83.52%; recall:  78.21%; FB1:  80.78\n              LOC: precision:  89.28%; recall:  85.68%; FB1:  87.44  1763\n             MISC: precision:  87.12%; recall:  77.01%; FB1:  81.75  815\n              ORG: precision:  74.26%; recall:  72.48%; FB1:  73.36  1309\n              PER: precision:  82.95%; recall:  75.52%; FB1:  79.06  1677\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Test Results Task 1 ","metadata":{}},{"cell_type":"code","source":"# Load the state dictionary\nmodel.load_state_dict(torch.load('task1_model.pt'))\nmodel.eval()\n# Move the model to the same device as the input data (cuda or cpu)\nmodel.to(device)\n\nwith torch.no_grad():\n    preds = []\n    real_labels = []\n    for batch in tqdm(test_loader):\n        test_input_ids, test_labels = batch['input_ids'].to(device, dtype=torch.long), batch['labels'].to(device, dtype=torch.long)\n        logits = model(test_input_ids)\n\n        predictions = torch.argmax(logits, dim=-1).cpu().numpy().tolist()\n        real_val_labels = test_labels.cpu().numpy().tolist()\n\n        for temp in range(len(batch['input_id_orig'])):\n            preds.append(predictions[temp][:batch['input_id_orig'][temp]])\n            real_labels.append(real_val_labels[temp][:batch['input_id_orig'][temp]])\n            \npreds = list(itertools.chain(*preds))\nreal_labels = list(itertools.chain(*real_labels))\n\npreds = [idx_to_tag[prediction] for prediction in preds]\nreal_labels = [idx_to_tag[label] for label in real_labels]\n\n# Evaluate on validation data and print the results\nmetrics = evaluate(real_labels, preds)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-11T02:10:11.699916Z","iopub.execute_input":"2023-11-11T02:10:11.700177Z","iopub.status.idle":"2023-11-11T02:10:12.820525Z","shell.execute_reply.started":"2023-11-11T02:10:11.700155Z","shell.execute_reply":"2023-11-11T02:10:12.819622Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  1.05it/s]","output_type":"stream"},{"name":"stdout","text":"processed 46435 tokens with 5648 phrases; found: 5185 phrases; correct: 3814.\naccuracy:  72.40%; (non-O)\naccuracy:  94.03%; precision:  73.56%; recall:  67.53%; FB1:  70.41\n              LOC: precision:  81.21%; recall:  77.76%; FB1:  79.45  1597\n             MISC: precision:  73.30%; recall:  64.53%; FB1:  68.64  618\n              ORG: precision:  67.07%; recall:  63.52%; FB1:  65.24  1573\n              PER: precision:  72.23%; recall:  62.40%; FB1:  66.95  1397\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"!wget http://nlp.stanford.edu/data/glove.6B.zip","metadata":{"execution":{"iopub.status.busy":"2023-11-11T02:15:04.883012Z","iopub.execute_input":"2023-11-11T02:15:04.883448Z","iopub.status.idle":"2023-11-11T02:17:45.562767Z","shell.execute_reply.started":"2023-11-11T02:15:04.883414Z","shell.execute_reply":"2023-11-11T02:17:45.561602Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"--2023-11-11 02:15:05--  http://nlp.stanford.edu/data/glove.6B.zip\nResolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\nConnecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://nlp.stanford.edu/data/glove.6B.zip [following]\n--2023-11-11 02:15:05--  https://nlp.stanford.edu/data/glove.6B.zip\nConnecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\nHTTP request sent, awaiting response... 301 Moved Permanently\nLocation: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n--2023-11-11 02:15:06--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\nResolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\nConnecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 862182613 (822M) [application/zip]\nSaving to: ‘glove.6B.zip.2’\n\nglove.6B.zip.2      100%[===================>] 822.24M  5.01MB/s    in 2m 39s  \n\n2023-11-11 02:17:45 (5.17 MB/s) - ‘glove.6B.zip.2’ saved [862182613/862182613]\n\n","output_type":"stream"}]},{"cell_type":"code","source":"!unzip glove.6B.zip","metadata":{"execution":{"iopub.status.busy":"2023-11-11T02:17:59.279639Z","iopub.execute_input":"2023-11-11T02:17:59.280540Z","iopub.status.idle":"2023-11-11T02:18:21.255966Z","shell.execute_reply.started":"2023-11-11T02:17:59.280507Z","shell.execute_reply":"2023-11-11T02:18:21.254799Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"Archive:  glove.6B.zip.1\n  inflating: glove.6B.50d.txt        \n  inflating: glove.6B.100d.txt       \n  inflating: glove.6B.200d.txt       \n  inflating: glove.6B.300d.txt       \n","output_type":"stream"}]},{"cell_type":"code","source":"# Initialize the word_dict dictionary\nword_dict = {'[PAD]': 0, '[UNK]': 1}\n\n# Initialize the embedding_matrix list\nembedding_matrix = []\n\n# Open and read the glove file\nwith open('glove.6B.100d.txt', 'r') as file:\n    for line in file:\n        # Split the line into words\n        line_words = line.split()\n\n        # Add the word to the dictionary and its corresponding embedding to the list\n        word_dict[line_words[0]] = len(word_dict)\n        embedding_matrix.append([float(x) for x in line_words[1:]])\n        \nembedding_dimension = 100\n\n# Insert zero vector at the beginning of embedding_matrix\nembedding_matrix.insert(0, np.zeros(embedding_dimension))\n\n# Insert the average vector at the beginning of embedding_matrix\nembedding_matrix.insert(1, np.average(np.asarray(embedding_matrix), axis=0))\n\n# Iterate through the keys in word_dict\nfor key in list(word_dict.keys()):\n    # Check if the key is alphabetic\n    if key.isalpha():\n        # Check if the capitalized form is not in the dictionary\n        if key.capitalize() not in word_dict.keys():\n            # Add the capitalized form to the dictionary and its corresponding vector to embedding_matrix\n            word_dict[key.capitalize()] = len(word_dict)\n            embedding_matrix.append(embedding_matrix[word_dict[key]])\n        \n        # Check if the uppercase form is not in the dictionary\n        if key.upper() not in word_dict.keys():\n            # Add the uppercase form to the dictionary and its corresponding vector to embedding_matrix\n            word_dict[key.upper()] = len(word_dict)\n            embedding_matrix.append(embedding_matrix[word_dict[key]])\n\n# Convert embedding_matrix to a NumPy array\nembedding_matrix = np.asarray(embedding_matrix)","metadata":{"execution":{"iopub.status.busy":"2023-11-11T02:18:31.524664Z","iopub.execute_input":"2023-11-11T02:18:31.525466Z","iopub.status.idle":"2023-11-11T02:19:01.947373Z","shell.execute_reply.started":"2023-11-11T02:18:31.525428Z","shell.execute_reply":"2023-11-11T02:19:01.946534Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"# Preprocess the dataset using the provided word2idx mapping\ndef preprocess_sample_glove(sample):\n    # Convert tokens to their respective indexes using w2ids\n    glove_input_ids = [word_dict.get(word, word_dict['[UNK]']) for word in sample['tokens']]\n    \n    # Update the sample with 'input_ids'\n    sample['glove_input_ids'] = glove_input_ids\n    \n    # Remove 'pos tags' and 'chunk tags'\n    sample.pop('pos_tags', None)\n    sample.pop('chunk_tags', None)\n    sample.pop('id', None)\n    \n    # Rename 'ner_tags' to 'labels'\n    sample['labels'] = sample.pop('ner_tags')\n    \n    return sample\n\n# Apply the preprocessing using .map() function\npreprocessed_glove_dataset = dataset.map(preprocess_sample_glove)","metadata":{"execution":{"iopub.status.busy":"2023-11-11T02:19:01.949319Z","iopub.execute_input":"2023-11-11T02:19:01.949628Z","iopub.status.idle":"2023-11-11T02:19:06.063459Z","shell.execute_reply.started":"2023-11-11T02:19:01.949602Z","shell.execute_reply":"2023-11-11T02:19:06.062540Z"},"trusted":true},"execution_count":33,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/14042 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"63014083be694700b57c04863aef7c26"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3251 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"72fd613178eb4e49a0785a440271748b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3454 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9cfb2b2ad2e1456591a93f8374ea9b8f"}},"metadata":{}}]},{"cell_type":"code","source":"preprocessed_glove_dataset","metadata":{"execution":{"iopub.status.busy":"2023-11-11T02:19:06.064899Z","iopub.execute_input":"2023-11-11T02:19:06.065526Z","iopub.status.idle":"2023-11-11T02:19:06.071776Z","shell.execute_reply.started":"2023-11-11T02:19:06.065488Z","shell.execute_reply":"2023-11-11T02:19:06.070708Z"},"trusted":true},"execution_count":34,"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['tokens', 'glove_input_ids', 'labels'],\n        num_rows: 14042\n    })\n    validation: Dataset({\n        features: ['tokens', 'glove_input_ids', 'labels'],\n        num_rows: 3251\n    })\n    test: Dataset({\n        features: ['tokens', 'glove_input_ids', 'labels'],\n        num_rows: 3454\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"# Assuming you have a preprocessed train, test, and validation dataset\ntrain_dataset = preprocessed_glove_dataset['train']\ntest_dataset = preprocessed_glove_dataset['test']\nvalidation_dataset = preprocessed_glove_dataset['validation']\n\n# Define the special label for 'PAD'\nPAD_LABEL = 9\n\n# Create custom collate function for DataLoader\ndef custom_collate(batch):\n    # Separate input_ids and labels\n    glove_input_ids = [torch.tensor(item['glove_input_ids']) for item in batch]\n    labels = [torch.tensor(item['labels']) for item in batch]\n    input_id_orig = [len(terms) for terms in glove_input_ids]\n    \n    # Pad input_ids and labels using pad_sequence\n    glove_input_ids = pad_sequence(glove_input_ids, batch_first=True, padding_value=word_dict['[PAD]'])\n    labels = pad_sequence(labels, batch_first=True, padding_value=PAD_LABEL)\n\n    return {'glove_input_ids': glove_input_ids, 'labels': labels, 'input_id_orig': input_id_orig}\n\n# Create DataLoader for train, test, and validation datasets\nbatch_size = 32  # You can adjust the batch size as needed\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=custom_collate, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=len(test_dataset), collate_fn=custom_collate)\nvalidation_loader = DataLoader(validation_dataset, batch_size=batch_size, collate_fn=custom_collate)","metadata":{"execution":{"iopub.status.busy":"2023-11-11T02:19:06.073011Z","iopub.execute_input":"2023-11-11T02:19:06.073301Z","iopub.status.idle":"2023-11-11T02:19:06.097695Z","shell.execute_reply.started":"2023-11-11T02:19:06.073272Z","shell.execute_reply":"2023-11-11T02:19:06.096815Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"# Define the BiLSTM model\nclass BiLSTMModel(nn.Module):\n    def __init__(self, glove_embedding_matrix, embedding_dim, num_lstm_layers, lstm_hidden_dim, linear_output_dim, tagset_size):\n        super(BiLSTMModel, self).__init__()\n        self.embedding = nn.Embedding.from_pretrained(torch.from_numpy(glove_embedding_matrix), freeze=False)\n        self.bilstm = nn.LSTM(embedding_dim, lstm_hidden_dim, num_layers=num_lstm_layers, bidirectional=True, batch_first=True)\n        self.linear = nn.Linear(2 * lstm_hidden_dim, linear_output_dim)\n        self.elu = nn.ELU()\n        self.classifier = nn.Linear(linear_output_dim, tagset_size)\n        self.dropout = nn.Dropout(p=0.33)  # Adjust the dropout rate as needed\n        \n    def forward(self, glove_input_ids):\n        embeddings = self.embedding(glove_input_ids)\n        # Ensure the data type of the embeddings matches the expected data type for the LSTM layer\n        embeddings = embeddings.to(torch.float32)  # Change torch.float32 to the correct data type\n\n        lstm_out, _ = self.bilstm(embeddings)\n        lstm_out = self.dropout(lstm_out)  # Apply dropout to the LSTM output\n        linear_out = self.linear(lstm_out)\n        elu_out = self.elu(linear_out)\n        logits = self.classifier(elu_out)\n        return logits\n    \n# Check for GPU availability\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n# Define hyperparameters\nglove_embedding_matrix = embedding_matrix\ntagset_size = 9\nembedding_dim = 100\nnum_lstm_layers = 1\nlstm_hidden_dim = 256\nlinear_output_dim = 128\nlearning_rate = 0.001\nnum_epochs = 100  # You can adjust the number of epochs\n\n# Create BiLSTM model\nmodel = BiLSTMModel(glove_embedding_matrix, embedding_dim, num_lstm_layers, lstm_hidden_dim, linear_output_dim, tagset_size)\nmodel.to(device)\n\n# Define loss function and optimizer\ncriterion = nn.CrossEntropyLoss(ignore_index=9)  # Ignore the pad token\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n# Early stopping parameters\npatience = 5  # Number of epochs with no improvement before stopping\nbest_validation_loss = float('inf')\ncounter = 0\nbest_validation_loss = 0\n\nidx_to_tag = {0:'O', 1:'B-PER', 2:'I-PER', 3:'B-ORG', 4:'I-ORG', 5:'B-LOC', 6:'I-LOC', 7:'B-MISC', 8:'I-MISC'}\n\ndef train(model, train_loader, optimizer, criterion, idx_to_tag):\n    model.train()\n\n    for batch in train_loader:\n        glove_input_ids, labels = batch['glove_input_ids'].to(device, dtype=torch.long), batch['labels'].to(device, dtype=torch.long)\n        optimizer.zero_grad()\n        logits = model(glove_input_ids)\n        loss = criterion(logits.view(-1, tagset_size), labels.view(-1))\n        loss.backward()\n        optimizer.step()\n\ndef eval_model(model, loader, idx_to_tag):\n    model.eval()\n\n    with torch.no_grad():\n        preds = []\n        real_labels = []\n        for batch in tqdm(loader):\n            glove_val_input_ids, val_labels = batch['glove_input_ids'].to(device, dtype=torch.long), batch['labels'].to(device, dtype=torch.long)\n            logits = model(glove_val_input_ids)\n\n            predictions = torch.argmax(logits, dim=-1).cpu().numpy().tolist()\n            real_val_labels = val_labels.cpu().numpy().tolist()\n            \n            for temp in range(len(batch['input_id_orig'])):\n                preds.append(predictions[temp][:batch['input_id_orig'][temp]])\n                real_labels.append(real_val_labels[temp][:batch['input_id_orig'][temp]])\n            \n    preds = list(itertools.chain(*preds))\n    real_labels = list(itertools.chain(*real_labels))\n\n    preds = [idx_to_tag[prediction] for prediction in preds]\n    real_labels = [idx_to_tag[label] for label in real_labels]\n\n    # Evaluate on validation data and print the results\n    metrics = evaluate(real_labels, preds)\n\n    return metrics\n\n# Training loop\nfor epoch in range(num_epochs):\n    train(model, train_loader, optimizer, criterion, idx_to_tag)\n\n    print(f\"Epoch {epoch+1}:\")\n    # Set the model to evaluation mode for validation\n    val_loss = eval_model(model, validation_loader, idx_to_tag)\n\n    # Early stopping check\n    if val_loss[2] > best_validation_loss:\n        best_validation_loss = val_loss[2]\n        counter = 0\n        # Save the model as .pt\n        torch.save(model.state_dict(), 'task2_model.pt')\n    else:\n        counter += 1\n        if counter >= patience:\n            print(f'Early stopping at epoch {epoch+1}')\n            print(f'Best F1 score: {best_validation_loss}')\n            break","metadata":{"execution":{"iopub.status.busy":"2023-11-11T02:19:06.099712Z","iopub.execute_input":"2023-11-11T02:19:06.100015Z","iopub.status.idle":"2023-11-11T02:26:41.698819Z","shell.execute_reply.started":"2023-11-11T02:19:06.099992Z","shell.execute_reply":"2023-11-11T02:26:41.697796Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"Epoch 1:\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 102/102 [00:00<00:00, 126.41it/s]\n","output_type":"stream"},{"name":"stdout","text":"processed 51362 tokens with 5942 phrases; found: 5703 phrases; correct: 4794.\naccuracy:  81.83%; (non-O)\naccuracy:  96.63%; precision:  84.06%; recall:  80.68%; FB1:  82.34\n              LOC: precision:  87.57%; recall:  85.52%; FB1:  86.53  1794\n             MISC: precision:  82.30%; recall:  67.57%; FB1:  74.21  757\n              ORG: precision:  72.30%; recall:  73.97%; FB1:  73.13  1372\n              PER: precision:  90.34%; recall:  87.30%; FB1:  88.79  1780\nEpoch 2:\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 102/102 [00:00<00:00, 127.53it/s]\n","output_type":"stream"},{"name":"stdout","text":"processed 51362 tokens with 5942 phrases; found: 6003 phrases; correct: 5165.\naccuracy:  88.00%; (non-O)\naccuracy:  97.52%; precision:  86.04%; recall:  86.92%; FB1:  86.48\n              LOC: precision:  90.04%; recall:  90.58%; FB1:  90.31  1848\n             MISC: precision:  75.15%; recall:  79.07%; FB1:  77.06  970\n              ORG: precision:  79.79%; recall:  80.69%; FB1:  80.24  1356\n              PER: precision:  92.40%; recall:  91.75%; FB1:  92.07  1829\nEpoch 3:\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 102/102 [00:00<00:00, 126.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"processed 51362 tokens with 5942 phrases; found: 6049 phrases; correct: 5257.\naccuracy:  89.93%; (non-O)\naccuracy:  97.75%; precision:  86.91%; recall:  88.47%; FB1:  87.68\n              LOC: precision:  92.16%; recall:  90.91%; FB1:  91.53  1812\n             MISC: precision:  75.45%; recall:  81.02%; FB1:  78.14  990\n              ORG: precision:  81.30%; recall:  83.67%; FB1:  82.47  1380\n              PER: precision:  92.02%; recall:  93.27%; FB1:  92.64  1867\nEpoch 4:\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 102/102 [00:00<00:00, 128.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"processed 51362 tokens with 5942 phrases; found: 5869 phrases; correct: 5232.\naccuracy:  88.69%; (non-O)\naccuracy:  97.81%; precision:  89.15%; recall:  88.05%; FB1:  88.60\n              LOC: precision:  94.17%; recall:  89.66%; FB1:  91.86  1749\n             MISC: precision:  83.22%; recall:  81.24%; FB1:  82.22  900\n              ORG: precision:  81.43%; recall:  83.37%; FB1:  82.39  1373\n              PER: precision:  93.02%; recall:  93.27%; FB1:  93.14  1847\nEpoch 5:\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 102/102 [00:00<00:00, 123.39it/s]\n","output_type":"stream"},{"name":"stdout","text":"processed 51362 tokens with 5942 phrases; found: 5845 phrases; correct: 5204.\naccuracy:  88.49%; (non-O)\naccuracy:  97.77%; precision:  89.03%; recall:  87.58%; FB1:  88.30\n              LOC: precision:  92.75%; recall:  90.53%; FB1:  91.63  1793\n             MISC: precision:  81.40%; recall:  82.10%; FB1:  81.75  930\n              ORG: precision:  83.04%; recall:  83.22%; FB1:  83.13  1344\n              PER: precision:  93.81%; recall:  90.55%; FB1:  92.15  1778\nEpoch 6:\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 102/102 [00:00<00:00, 129.83it/s]\n","output_type":"stream"},{"name":"stdout","text":"processed 51362 tokens with 5942 phrases; found: 5918 phrases; correct: 5248.\naccuracy:  88.95%; (non-O)\naccuracy:  97.81%; precision:  88.68%; recall:  88.32%; FB1:  88.50\n              LOC: precision:  93.06%; recall:  90.58%; FB1:  91.81  1788\n             MISC: precision:  79.39%; recall:  82.32%; FB1:  80.83  956\n              ORG: precision:  83.86%; recall:  83.30%; FB1:  83.58  1332\n              PER: precision:  92.73%; recall:  92.73%; FB1:  92.73  1842\nEpoch 7:\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 102/102 [00:00<00:00, 124.84it/s]\n","output_type":"stream"},{"name":"stdout","text":"processed 51362 tokens with 5942 phrases; found: 5965 phrases; correct: 5264.\naccuracy:  89.74%; (non-O)\naccuracy:  97.85%; precision:  88.25%; recall:  88.59%; FB1:  88.42\n              LOC: precision:  94.31%; recall:  90.20%; FB1:  92.21  1757\n             MISC: precision:  84.12%; recall:  81.56%; FB1:  82.82  894\n              ORG: precision:  79.10%; recall:  85.23%; FB1:  82.05  1445\n              PER: precision:  91.60%; recall:  92.94%; FB1:  92.27  1869\nEpoch 8:\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 102/102 [00:00<00:00, 127.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"processed 51362 tokens with 5942 phrases; found: 6039 phrases; correct: 5277.\naccuracy:  89.74%; (non-O)\naccuracy:  97.77%; precision:  87.38%; recall:  88.81%; FB1:  88.09\n              LOC: precision:  91.79%; recall:  91.34%; FB1:  91.57  1828\n             MISC: precision:  81.28%; recall:  81.02%; FB1:  81.15  919\n              ORG: precision:  79.89%; recall:  84.41%; FB1:  82.09  1417\n              PER: precision:  91.73%; recall:  93.38%; FB1:  92.55  1875\nEpoch 9:\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 102/102 [00:00<00:00, 130.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"processed 51362 tokens with 5942 phrases; found: 6045 phrases; correct: 5277.\naccuracy:  89.50%; (non-O)\naccuracy:  97.76%; precision:  87.30%; recall:  88.81%; FB1:  88.05\n              LOC: precision:  92.37%; recall:  89.60%; FB1:  90.96  1782\n             MISC: precision:  79.33%; recall:  82.00%; FB1:  80.64  953\n              ORG: precision:  80.90%; recall:  85.61%; FB1:  83.19  1419\n              PER: precision:  91.33%; recall:  93.76%; FB1:  92.53  1891\nEarly stopping at epoch 9\nBest F1 score: 88.59537719075439\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Validation Results Task 2","metadata":{}},{"cell_type":"code","source":"# Load the state dictionary\nmodel.load_state_dict(torch.load('task2_model.pt'))\nmodel.eval()\n# Move the model to the same device as the input data (cuda or cpu)\nmodel.to(device)\n\nwith torch.no_grad():\n    preds = []\n    real_labels = []\n    for batch in tqdm(validation_loader):\n        val_glove_input_ids, val_labels = batch['glove_input_ids'].to(device, dtype=torch.long), batch['labels'].to(device, dtype=torch.long)\n        logits = model(val_glove_input_ids)\n\n        predictions = torch.argmax(logits, dim=-1).cpu().numpy().tolist()\n        real_val_labels = val_labels.cpu().numpy().tolist()\n\n        for temp in range(len(batch['input_id_orig'])):\n            preds.append(predictions[temp][:batch['input_id_orig'][temp]])\n            real_labels.append(real_val_labels[temp][:batch['input_id_orig'][temp]])\n            \npreds = list(itertools.chain(*preds))\nreal_labels = list(itertools.chain(*real_labels))\n\npreds = [idx_to_tag[prediction] for prediction in preds]\nreal_labels = [idx_to_tag[label] for label in real_labels]\n\n# Evaluate on validation data and print the results\nmetrics = evaluate(real_labels, preds)","metadata":{"execution":{"iopub.status.busy":"2023-11-11T02:26:41.700272Z","iopub.execute_input":"2023-11-11T02:26:41.700733Z","iopub.status.idle":"2023-11-11T02:26:43.289851Z","shell.execute_reply.started":"2023-11-11T02:26:41.700697Z","shell.execute_reply":"2023-11-11T02:26:43.288773Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stderr","text":"100%|██████████| 102/102 [00:00<00:00, 142.27it/s]\n","output_type":"stream"},{"name":"stdout","text":"processed 51362 tokens with 5942 phrases; found: 5869 phrases; correct: 5232.\naccuracy:  88.69%; (non-O)\naccuracy:  97.81%; precision:  89.15%; recall:  88.05%; FB1:  88.60\n              LOC: precision:  94.17%; recall:  89.66%; FB1:  91.86  1749\n             MISC: precision:  83.22%; recall:  81.24%; FB1:  82.22  900\n              ORG: precision:  81.43%; recall:  83.37%; FB1:  82.39  1373\n              PER: precision:  93.02%; recall:  93.27%; FB1:  93.14  1847\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Test Results Task 2","metadata":{}},{"cell_type":"code","source":"# Load the state dictionary\nmodel.load_state_dict(torch.load('task2_model.pt'))\nmodel.eval()\n# Move the model to the same device as the input data (cuda or cpu)\nmodel.to(device)\n\nwith torch.no_grad():\n    preds = []\n    real_labels = []\n    for batch in tqdm(test_loader):\n        test_glove_input_ids, test_labels = batch['glove_input_ids'].to(device, dtype=torch.long), batch['labels'].to(device, dtype=torch.long)\n        logits = model(test_glove_input_ids)\n\n        predictions = torch.argmax(logits, dim=-1).cpu().numpy().tolist()\n        real_val_labels = test_labels.cpu().numpy().tolist()\n\n        for temp in range(len(batch['input_id_orig'])):\n            preds.append(predictions[temp][:batch['input_id_orig'][temp]])\n            real_labels.append(real_val_labels[temp][:batch['input_id_orig'][temp]])\n            \npreds = list(itertools.chain(*preds))\nreal_labels = list(itertools.chain(*real_labels))\n\npreds = [idx_to_tag[prediction] for prediction in preds]\nreal_labels = [idx_to_tag[label] for label in real_labels]\n\n# Evaluate on validation data and print the results\nmetrics = evaluate(real_labels, preds)","metadata":{"execution":{"iopub.status.busy":"2023-11-11T02:26:43.291270Z","iopub.execute_input":"2023-11-11T02:26:43.291697Z","iopub.status.idle":"2023-11-11T02:26:44.959983Z","shell.execute_reply.started":"2023-11-11T02:26:43.291657Z","shell.execute_reply":"2023-11-11T02:26:44.959043Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  1.26it/s]","output_type":"stream"},{"name":"stdout","text":"processed 46435 tokens with 5648 phrases; found: 5734 phrases; correct: 4720.\naccuracy:  85.92%; (non-O)\naccuracy:  96.64%; precision:  82.32%; recall:  83.57%; FB1:  82.94\n              LOC: precision:  88.55%; recall:  88.07%; FB1:  88.31  1659\n             MISC: precision:  69.69%; recall:  73.36%; FB1:  71.48  739\n              ORG: precision:  75.07%; recall:  78.87%; FB1:  76.92  1745\n              PER: precision:  89.63%; recall:  88.19%; FB1:  88.90  1591\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]}]}